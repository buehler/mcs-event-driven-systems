[INFO] Scanning for projects...
[INFO]
[INFO] ----------------< com.experiments:fault-tolerance-test >----------------
[INFO] Building fault-tolerance-test 1.0-SNAPSHOT
[INFO]   from pom.xml
[INFO] --------------------------------[ jar ]---------------------------------
[INFO]
[INFO] --- resources:3.3.1:resources (default-resources) @ fault-tolerance-test ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /Users/Simon_1/Documents/HSG/MCS/FS25/Event-driven and Process-oriented Architectures/Project/exercise1/faultToleranceTest/src/main/resources
[INFO]
[INFO] --- compiler:3.13.0:compile (default-compile) @ fault-tolerance-test ---
[INFO] Nothing to compile - all classes are up to date.
[INFO]
[INFO] --- resources:3.3.1:testResources (default-testResources) @ fault-tolerance-test ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 4 resources from src/test/resources to target/test-classes
[INFO]
[INFO] --- compiler:3.13.0:testCompile (default-testCompile) @ fault-tolerance-test ---
[INFO] Nothing to compile - all classes are up to date.
[INFO]
[INFO] --- surefire:3.1.2:test (default-test) @ fault-tolerance-test ---
[INFO] Using auto detected provider org.apache.maven.surefire.junitplatform.JUnitPlatformProvider
[INFO]
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.experiments.KafkaFailoverDockerTest
11:01:04,204 |-INFO in ch.qos.logback.classic.LoggerContext[default] - This is logback-classic version 1.4.8
11:01:04,214 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback-test.xml]
11:01:04,215 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Found resource [logback.xml] at [file:/Users/Simon_1/Documents/HSG/MCS/FS25/Event-driven%20and%20Process-oriented%20Architectures/Project/exercise1/faultToleranceTest/target/test-classes/logback.xml]
11:01:04,244 |-INFO in ch.qos.logback.core.model.processor.AppenderModelHandler - Processing appender named [CONSOLE]
11:01:04,244 |-INFO in ch.qos.logback.core.model.processor.AppenderModelHandler - About to instantiate appender of type [ch.qos.logback.core.ConsoleAppender]
11:01:04,245 |-INFO in ch.qos.logback.core.model.processor.ImplicitModelHandler - Assuming default type [ch.qos.logback.classic.encoder.PatternLayoutEncoder] for [encoder] property
11:01:04,252 |-INFO in ch.qos.logback.core.model.processor.AppenderModelHandler - Processing appender named [FILE]
11:01:04,252 |-INFO in ch.qos.logback.core.model.processor.AppenderModelHandler - About to instantiate appender of type [ch.qos.logback.core.FileAppender]
11:01:04,254 |-INFO in ch.qos.logback.core.model.processor.ImplicitModelHandler - Assuming default type [ch.qos.logback.classic.encoder.PatternLayoutEncoder] for [encoder] property
11:01:04,254 |-ERROR in ch.qos.logback.core.model.processor.ImplicitModelHandler - Could not create component [filter] of type [ch.qos.logback.classic.filter.MarkerFilter] java.lang.ClassNotFoundException: ch.qos.logback.classic.filter.MarkerFilter
        at java.lang.ClassNotFoundException: ch.qos.logback.classic.filter.MarkerFilter
        at      at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
        at      at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
        at      at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:528)
        at      at ch.qos.logback.core.util.Loader.loadClass(Loader.java:118)
        at      at ch.qos.logback.core.model.processor.ImplicitModelHandler.doComplex(ImplicitModelHandler.java:134)
        at      at ch.qos.logback.core.model.processor.ImplicitModelHandler.handle(ImplicitModelHandler.java:94)
        at      at ch.qos.logback.core.model.processor.DefaultProcessor.secondPhaseTraverse(DefaultProcessor.java:241)
        at      at ch.qos.logback.core.model.processor.DefaultProcessor.secondPhaseTraverse(DefaultProcessor.java:253)
        at      at ch.qos.logback.core.model.processor.DefaultProcessor.secondPhaseTraverse(DefaultProcessor.java:253)
        at      at ch.qos.logback.core.model.processor.DefaultProcessor.traversalLoop(DefaultProcessor.java:90)
        at      at ch.qos.logback.core.model.processor.DefaultProcessor.process(DefaultProcessor.java:106)
        at      at ch.qos.logback.core.joran.GenericXMLConfigurator.processModel(GenericXMLConfigurator.java:210)
        at      at ch.qos.logback.core.joran.GenericXMLConfigurator.doConfigure(GenericXMLConfigurator.java:174)
        at      at ch.qos.logback.core.joran.GenericXMLConfigurator.doConfigure(GenericXMLConfigurator.java:126)
        at      at ch.qos.logback.core.joran.GenericXMLConfigurator.doConfigure(GenericXMLConfigurator.java:69)
        at      at ch.qos.logback.classic.util.DefaultJoranConfigurator.configureByResource(DefaultJoranConfigurator.java:53)
        at      at ch.qos.logback.classic.util.DefaultJoranConfigurator.configure(DefaultJoranConfigurator.java:34)
        at      at ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:98)
        at      at ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:77)
        at      at ch.qos.logback.classic.spi.LogbackServiceProvider.initializeLoggerContext(LogbackServiceProvider.java:52)
        at      at ch.qos.logback.classic.spi.LogbackServiceProvider.initialize(LogbackServiceProvider.java:41)
        at      at org.slf4j.LoggerFactory.bind(LoggerFactory.java:183)
        at      at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:170)
        at      at org.slf4j.LoggerFactory.getProvider(LoggerFactory.java:455)
        at      at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:441)
        at      at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:390)
        at      at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:416)
        at      at com.experiments.KafkaFailoverDockerTest.<clinit>(KafkaFailoverDockerTest.java:43)
        at      at java.base/jdk.internal.misc.Unsafe.ensureClassInitialized0(Native Method)
        at      at java.base/jdk.internal.misc.Unsafe.ensureClassInitialized(Unsafe.java:1161)
        at      at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.ensureClassInitialized(MethodHandleAccessorFactory.java:340)
        at      at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newConstructorAccessor(MethodHandleAccessorFactory.java:103)
        at      at java.base/jdk.internal.reflect.ReflectionFactory.newConstructorAccessor(ReflectionFactory.java:173)
        at      at java.base/java.lang.reflect.Constructor.acquireConstructorAccessor(Constructor.java:548)
        at      at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:498)
        at      at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)
        at      at org.junit.platform.commons.util.ReflectionUtils.newInstance(ReflectionUtils.java:553)
        at      at org.junit.jupiter.engine.execution.ConstructorInvocation.proceed(ConstructorInvocation.java:56)
        at      at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
        at      at org.junit.jupiter.api.extension.InvocationInterceptor.interceptTestClassConstructor(InvocationInterceptor.java:74)
        at      at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
        at      at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
        at      at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
        at      at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
        at      at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
        at      at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
        at      at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:62)
        at      at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestClassConstructor(ClassBasedTestDescriptor.java:364)
        at      at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateTestClass(ClassBasedTestDescriptor.java:311)
        at      at org.junit.jupiter.engine.descriptor.ClassTestDescriptor.instantiateTestClass(ClassTestDescriptor.java:79)
        at      at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:287)
        at      at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:279)
        at      at java.base/java.util.Optional.orElseGet(Optional.java:364)
        at      at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:278)
        at      at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
        at      at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$before$2(ClassBasedTestDescriptor.java:204)
        at      at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at      at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:203)
        at      at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:85)
        at      at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
        at      at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at      at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at      at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at      at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at      at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at      at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at      at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at      at java.base/java.util.ArrayList.forEach(ArrayList.java:1597)
        at      at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
        at      at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
        at      at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at      at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
        at      at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
        at      at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
        at      at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
        at      at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
        at      at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
        at      at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
        at      at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
        at      at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
        at      at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:198)
        at      at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:169)
        at      at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:93)
        at      at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:58)
        at      at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:141)
        at      at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:57)
        at      at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:103)
        at      at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:85)
        at      at org.junit.platform.launcher.core.DelegatingLauncher.execute(DelegatingLauncher.java:47)
        at      at org.apache.maven.surefire.junitplatform.LazyLauncher.execute(LazyLauncher.java:56)
        at      at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:184)
        at      at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:148)
        at      at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:122)
        at      at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:385)
        at      at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
        at      at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:507)
        at      at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:495)
11:01:04,254 |-WARN in ch.qos.logback.core.model.processor.ImplicitModelHandler - Ignoring unknown property [marker] in [ch.qos.logback.core.FileAppender]
11:01:04,254 |-WARN in ch.qos.logback.core.model.processor.ImplicitModelHandler - Ignoring unknown property [onMatch] in [ch.qos.logback.core.FileAppender]
11:01:04,254 |-WARN in ch.qos.logback.core.model.processor.ImplicitModelHandler - Ignoring unknown property [onMismatch] in [ch.qos.logback.core.FileAppender]
11:01:04,254 |-INFO in ch.qos.logback.core.FileAppender[FILE] - File property is set to [logs/exercise1_faultToleranceTest_2025-04-16_11-01-04.log]
11:01:04,255 |-INFO in ch.qos.logback.classic.model.processor.RootLoggerModelHandler - Setting level of ROOT logger to INFO
11:01:04,255 |-INFO in ch.qos.logback.core.model.processor.AppenderRefModelHandler - Attaching appender named [CONSOLE] to Logger[ROOT]
11:01:04,255 |-INFO in ch.qos.logback.classic.model.processor.LoggerModelHandler - Setting level of logger [com.experiments] to INFO
11:01:04,256 |-INFO in ch.qos.logback.classic.model.processor.LoggerModelHandler - Setting additivity of logger [com.experiments] to false
11:01:04,256 |-INFO in ch.qos.logback.core.model.processor.AppenderRefModelHandler - Attaching appender named [FILE] to Logger[com.experiments]
11:01:04,256 |-INFO in ch.qos.logback.core.model.processor.DefaultProcessor@32115b28 - End of configuration.
11:01:04,256 |-INFO in ch.qos.logback.classic.joran.JoranConfigurator@2ad48653 - Registering current configuration as safe fallback point

2025-04-16 11:01:04.269;AdminClientConfig values:
        auto.include.jmx.reporter = true
        bootstrap.controllers = []
        bootstrap.servers = [192.168.1.173:9092, 192.168.1.173:9093, 192.168.1.173:9094]
        client.dns.lookup = use_all_dns_ips
        client.id =
        connections.max.idle.ms = 300000
        default.api.timeout.ms = 60000
        enable.metrics.push = true
        metadata.max.age.ms = 300000
        metadata.recovery.strategy = none
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.max.ms = 1000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.connect.timeout.ms = null
        sasl.login.read.timeout.ms = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.login.retry.backoff.max.ms = 10000
        sasl.login.retry.backoff.ms = 100
        sasl.mechanism = GSSAPI
        sasl.oauthbearer.clock.skew.seconds = 30
        sasl.oauthbearer.expected.audience = null
        sasl.oauthbearer.expected.issuer = null
        sasl.oauthbearer.header.urlencode = false
        sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
        sasl.oauthbearer.jwks.endpoint.url = null
        sasl.oauthbearer.scope.claim.name = scope
        sasl.oauthbearer.sub.claim.name = sub
        sasl.oauthbearer.token.endpoint.url = null
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.3
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS

2025-04-16 11:01:04.331;Kafka version: 3.9.0
2025-04-16 11:01:04.331;Kafka commitId: 84caaa6e9da06435
2025-04-16 11:01:04.331;Kafka startTimeMs: 1744794064330
Deleting all existing topics...
No topics found to delete.
Topic [test-replication-topic] created with 3 partitions and replication factor 3.
Waiting for topic metadata to propagate...
2025-04-16 11:01:14.612;ProducerConfig values:
        acks = -1
        auto.include.jmx.reporter = true
        batch.size = 16384
        bootstrap.servers = [192.168.1.173:9092, 192.168.1.173:9093, 192.168.1.173:9094]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-2
        compression.gzip.level = -1
        compression.lz4.level = 9
        compression.type = none
        compression.zstd.level = 3
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 30000
        enable.idempotence = true
        enable.metrics.push = true
        interceptor.classes = []
        key.serializer = class org.apache.kafka.common.serialization.StringSerializer
        linger.ms = 5
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 5
        max.request.size = 1048576
        metadata.max.age.ms = 1000
        metadata.max.idle.ms = 300000
        metadata.recovery.strategy = none
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.adaptive.partitioning.enable = true
        partitioner.availability.timeout.ms = 0
        partitioner.class = null
        partitioner.ignore.keys = false
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 10000
        reconnect.backoff.ms = 500
        request.timeout.ms = 15000
        retries = 3
        retry.backoff.max.ms = 1000
        retry.backoff.ms = 500
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.connect.timeout.ms = null
        sasl.login.read.timeout.ms = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.login.retry.backoff.max.ms = 10000
        sasl.login.retry.backoff.ms = 100
        sasl.mechanism = GSSAPI
        sasl.oauthbearer.clock.skew.seconds = 30
        sasl.oauthbearer.expected.audience = null
        sasl.oauthbearer.expected.issuer = null
        sasl.oauthbearer.header.urlencode = false
        sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
        sasl.oauthbearer.jwks.endpoint.url = null
        sasl.oauthbearer.scope.claim.name = scope
        sasl.oauthbearer.sub.claim.name = sub
        sasl.oauthbearer.token.endpoint.url = null
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.3
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-16 11:01:14.612;ProducerConfig values:
        acks = -1
        auto.include.jmx.reporter = true
        batch.size = 16384
        bootstrap.servers = [192.168.1.173:9092, 192.168.1.173:9093, 192.168.1.173:9094]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-3
        compression.gzip.level = -1
        compression.lz4.level = 9
        compression.type = none
        compression.zstd.level = 3
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 30000
        enable.idempotence = true
        enable.metrics.push = true
        interceptor.classes = []
        key.serializer = class org.apache.kafka.common.serialization.StringSerializer
        linger.ms = 5
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 5
        max.request.size = 1048576
        metadata.max.age.ms = 1000
        metadata.max.idle.ms = 300000
        metadata.recovery.strategy = none
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.adaptive.partitioning.enable = true
        partitioner.availability.timeout.ms = 0
        partitioner.class = null
        partitioner.ignore.keys = false
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 10000
        reconnect.backoff.ms = 500
        request.timeout.ms = 15000
        retries = 3
        retry.backoff.max.ms = 1000
        retry.backoff.ms = 500
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.connect.timeout.ms = null
        sasl.login.read.timeout.ms = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.login.retry.backoff.max.ms = 10000
        sasl.login.retry.backoff.ms = 100
        sasl.mechanism = GSSAPI
        sasl.oauthbearer.clock.skew.seconds = 30
        sasl.oauthbearer.expected.audience = null
        sasl.oauthbearer.expected.issuer = null
        sasl.oauthbearer.header.urlencode = false
        sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
        sasl.oauthbearer.jwks.endpoint.url = null
        sasl.oauthbearer.scope.claim.name = scope
        sasl.oauthbearer.sub.claim.name = sub
        sasl.oauthbearer.token.endpoint.url = null
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.3
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-16 11:01:14.612;ProducerConfig values:
        acks = -1
        auto.include.jmx.reporter = true
        batch.size = 16384
        bootstrap.servers = [192.168.1.173:9092, 192.168.1.173:9093, 192.168.1.173:9094]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-1
        compression.gzip.level = -1
        compression.lz4.level = 9
        compression.type = none
        compression.zstd.level = 3
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 30000
        enable.idempotence = true
        enable.metrics.push = true
        interceptor.classes = []
        key.serializer = class org.apache.kafka.common.serialization.StringSerializer
        linger.ms = 5
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 5
        max.request.size = 1048576
        metadata.max.age.ms = 1000
        metadata.max.idle.ms = 300000
        metadata.recovery.strategy = none
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.adaptive.partitioning.enable = true
        partitioner.availability.timeout.ms = 0
        partitioner.class = null
        partitioner.ignore.keys = false
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 10000
        reconnect.backoff.ms = 500
        request.timeout.ms = 15000
        retries = 3
        retry.backoff.max.ms = 1000
        retry.backoff.ms = 500
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.connect.timeout.ms = null
        sasl.login.read.timeout.ms = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.login.retry.backoff.max.ms = 10000
        sasl.login.retry.backoff.ms = 100
        sasl.mechanism = GSSAPI
        sasl.oauthbearer.clock.skew.seconds = 30
        sasl.oauthbearer.expected.audience = null
        sasl.oauthbearer.expected.issuer = null
        sasl.oauthbearer.header.urlencode = false
        sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
        sasl.oauthbearer.jwks.endpoint.url = null
        sasl.oauthbearer.scope.claim.name = scope
        sasl.oauthbearer.sub.claim.name = sub
        sasl.oauthbearer.token.endpoint.url = null
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.3
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-16 11:01:14.612;ConsumerConfig values:
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.include.jmx.reporter = true
        auto.offset.reset = latest
        bootstrap.servers = [192.168.1.173:9092, 192.168.1.173:9093, 192.168.1.173:9094]
        check.crcs = true
        client.dns.lookup = use_all_dns_ips
        client.id = consumer-test-consumer-group-1
        client.rack =
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        enable.metrics.push = true
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = test-consumer-group
        group.instance.id = null
        group.protocol = classic
        group.remote.assignor = null
        heartbeat.interval.ms = 1000
        interceptor.classes = []
        internal.leave.group.on.close = true
        internal.throw.on.fetch.stable.offset.unsupported = false
        isolation.level = read_uncommitted
        key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 60000
        max.poll.records = 500
        metadata.max.age.ms = 300000
        metadata.recovery.strategy = none
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 10000
        reconnect.backoff.ms = 1000
        request.timeout.ms = 40000
        retry.backoff.max.ms = 1000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.connect.timeout.ms = null
        sasl.login.read.timeout.ms = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.login.retry.backoff.max.ms = 10000
        sasl.login.retry.backoff.ms = 100
        sasl.mechanism = GSSAPI
        sasl.oauthbearer.clock.skew.seconds = 30
        sasl.oauthbearer.expected.audience = null
        sasl.oauthbearer.expected.issuer = null
        sasl.oauthbearer.header.urlencode = false
        sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
        sasl.oauthbearer.jwks.endpoint.url = null
        sasl.oauthbearer.scope.claim.name = scope
        sasl.oauthbearer.sub.claim.name = sub
        sasl.oauthbearer.token.endpoint.url = null
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        session.timeout.ms = 10000
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.3
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-16 11:01:14.630;initializing Kafka metrics collector
2025-04-16 11:01:14.630;initializing Kafka metrics collector
2025-04-16 11:01:14.630;initializing Kafka metrics collector
2025-04-16 11:01:14.631;initializing Kafka metrics collector
2025-04-16 11:01:14.636;[Producer clientId=producer-2] Instantiated an idempotent producer.
2025-04-16 11:01:14.636;[Producer clientId=producer-1] Instantiated an idempotent producer.
2025-04-16 11:01:14.636;[Producer clientId=producer-3] Instantiated an idempotent producer.
2025-04-16 11:01:14.646;Kafka version: 3.9.0
2025-04-16 11:01:14.646;Kafka commitId: 84caaa6e9da06435
2025-04-16 11:01:14.646;Kafka startTimeMs: 1744794074646
2025-04-16 11:01:14.646;Kafka version: 3.9.0
2025-04-16 11:01:14.646;Kafka commitId: 84caaa6e9da06435
2025-04-16 11:01:14.646;Kafka startTimeMs: 1744794074646
2025-04-16 11:01:14.646;Kafka version: 3.9.0
2025-04-16 11:01:14.646;Kafka commitId: 84caaa6e9da06435
2025-04-16 11:01:14.646;Kafka startTimeMs: 1744794074646
2025-04-16 11:01:14.653;Kafka version: 3.9.0
2025-04-16 11:01:14.653;Kafka commitId: 84caaa6e9da06435
2025-04-16 11:01:14.653;Kafka startTimeMs: 1744794074653
2025-04-16 11:01:14.653;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Subscribed to topic(s): test-replication-topic
Subscribed to topic: test-replication-topic
2025-04-16 11:01:14.663;[Producer clientId=producer-3] Cluster ID: oM0b0RdxS16dv1KzvYWuXw
2025-04-16 11:01:14.663;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Cluster ID: oM0b0RdxS16dv1KzvYWuXw
2025-04-16 11:01:14.663;[Producer clientId=producer-1] Cluster ID: oM0b0RdxS16dv1KzvYWuXw
2025-04-16 11:01:14.867;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Discovered group coordinator 192.168.1.173:9093 (id: 2147483645 rack: null)
2025-04-16 11:01:14.868;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] (Re-)joining group
2025-04-16 11:01:14.876;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Request joining group due to: need to re-join with the given member-id: consumer-test-consumer-group-1-bd2a143d-6e00-4b23-9770-c195b8a09701
2025-04-16 11:01:14.877;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] (Re-)joining group
2025-04-16 11:01:15.166;[Producer clientId=producer-2] Cluster ID: oM0b0RdxS16dv1KzvYWuXw
2025-04-16 11:01:15.173;[Producer clientId=producer-2] ProducerId set to 0 with epoch 0
2025-04-16 11:01:15.682;[Producer clientId=producer-3] ProducerId set to 2000 with epoch 0
2025-04-16 11:01:15.682;[Producer clientId=producer-1] ProducerId set to 2001 with epoch 0
2025-04-16 11:01:17.894;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Successfully joined group with generation Generation{generationId=1, memberId='consumer-test-consumer-group-1-bd2a143d-6e00-4b23-9770-c195b8a09701', protocol='range'}
2025-04-16 11:01:17.900;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Finished assignment for group at generation 1: {consumer-test-consumer-group-1-bd2a143d-6e00-4b23-9770-c195b8a09701=Assignment(partitions=[test-replication-topic-0, test-replication-topic-1, test-replication-topic-2])}
2025-04-16 11:01:17.911;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Successfully synced group in generation Generation{generationId=1, memberId='consumer-test-consumer-group-1-bd2a143d-6e00-4b23-9770-c195b8a09701', protocol='range'}
2025-04-16 11:01:17.912;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Notifying assignor about the new Assignment(partitions=[test-replication-topic-0, test-replication-topic-1, test-replication-topic-2])
2025-04-16 11:01:17.914;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Adding newly assigned partitions: test-replication-topic-0, test-replication-topic-1, test-replication-topic-2
Partitions assigned after rebalance: [test-replication-topic-0, test-replication-topic-1, test-replication-topic-2]
2025-04-16 11:01:17.920;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Found no committed offset for partition test-replication-topic-2
2025-04-16 11:01:17.920;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Found no committed offset for partition test-replication-topic-1
2025-04-16 11:01:17.920;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Found no committed offset for partition test-replication-topic-0
2025-04-16 11:01:17.927;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Resetting offset for partition test-replication-topic-2 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.173:9092 (id: 1 rack: null)], epoch=0}}.
2025-04-16 11:01:17.929;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Resetting offset for partition test-replication-topic-0 to position FetchPosition{offset=98, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.173:9093 (id: 2 rack: null)], epoch=0}}.
2025-04-16 11:01:17.930;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Resetting offset for partition test-replication-topic-1 to position FetchPosition{offset=73, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.173:9094 (id: 3 rack: null)], epoch=0}}.
Stopping broker [kafka1]...
Broker kafka1 stopped. Observing failover...
firstConsumedAfterNodeFailure set to: 1744794084636
firstProducedAfterNodeFailure set to: 1744794084627
All partitions have transitioned to new leaders:
Partition 0: New leader = 2
Partition 1: New leader = 3
Partition 2: New leader = 2
2025-04-16 11:01:24.679;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Node -1 disconnected.
2025-04-16 11:01:24.679;[Producer clientId=producer-1] Node 1 disconnected.
2025-04-16 11:01:24.679;[Producer clientId=producer-3] Node -1 disconnected.
2025-04-16 11:01:24.680;[Producer clientId=producer-2] Node 1 disconnected.
2025-04-16 11:01:24.681;[Producer clientId=producer-1] Node -1 disconnected.
2025-04-16 11:01:24.681;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Node 1 disconnected.
2025-04-16 11:01:24.681;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Cancelled in-flight METADATA request with correlation id 892 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, throttle time: 0ms, request timeout: 40000ms)
2025-04-16 11:01:24.681;[Producer clientId=producer-3] Node 1 disconnected.

Restarting broker 'kafka1'...
Working Directory: /Users/Simon_1/Documents/HSG/MCS/FS25/Event-driven and Process-oriented Architectures/Project/exercise1/faultToleranceTest/../docker
firstConsumedAfterNodeRecovery set to: 1744794094696
firstProducedAfterNodeRecovery set to: 1744794094686
 Container kafka1  Restarting
 Container kafka1  Started
Broker 'kafka1' restarted successfully.
Broker kafka1 restarted.
Stopping producer and consumer threads...
Producer thread stopped.
Producer thread stopped.
2025-04-16 11:01:44.822;[Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-04-16 11:01:44.822;[Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
Producer thread stopped.
2025-04-16 11:01:44.824;[Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2025-04-16 11:01:44.829;Metrics scheduler closed
2025-04-16 11:01:44.830;Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-16 11:01:44.830;Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-16 11:01:44.830;Metrics scheduler closed
2025-04-16 11:01:44.830;Metrics reporters closed
2025-04-16 11:01:44.830;Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-16 11:01:44.830;Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-16 11:01:44.830;Metrics reporters closed
2025-04-16 11:01:44.830;Metrics scheduler closed
2025-04-16 11:01:44.830;Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-16 11:01:44.830;Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-16 11:01:44.830;Metrics reporters closed
2025-04-16 11:01:44.830;App info kafka.producer for producer-3 unregistered
2025-04-16 11:01:44.830;App info kafka.producer for producer-2 unregistered
2025-04-16 11:01:44.830;App info kafka.producer for producer-1 unregistered
2025-04-16 11:01:44.897;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Revoke previously assigned partitions test-replication-topic-0, test-replication-topic-1, test-replication-topic-2
Partitions revoked. Preparing for reassignment: [test-replication-topic-0, test-replication-topic-1, test-replication-topic-2]
2025-04-16 11:01:44.897;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Member consumer-test-consumer-group-1-bd2a143d-6e00-4b23-9770-c195b8a09701 sending LeaveGroup request to coordinator 192.168.1.173:9093 (id: 2147483645 rack: null) due to the consumer is being closed
2025-04-16 11:01:44.897;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-16 11:01:44.897;[Consumer clientId=consumer-test-consumer-group-1, groupId=test-consumer-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-16 11:01:45.297;Metrics scheduler closed
2025-04-16 11:01:45.297;Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-16 11:01:45.297;Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-16 11:01:45.298;Metrics reporters closed
2025-04-16 11:01:45.302;App info kafka.consumer for consumer-test-consumer-group-1 unregistered
Producer and consumer threads stopped.
2025-04-16 11:01:45.310;App info kafka.admin.client for adminclient-1 unregistered
2025-04-16 11:01:45.311;Metrics scheduler closed
2025-04-16 11:01:45.311;Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-16 11:01:45.311;Metrics reporters closed

========== Kafka Failover Test Report ==========
Test Name: Kafka Broker Failover Test
Topic Name: test-replication-topic
Broker Kill Time (ms): 10
Leader Election Time (ms): 53
Broker Restart Time (ms): 131
Consumer Lag (Messages): 199
Total Produced Messages: 2057
Total Consumed Messages: 1858
Test Successful: true
Revoked Partitions Count: 3
Assigned Partitions Count: 3
Producer Recovery Time (Node Failure): 41 ms
Consumer Recovery Time (Node Failure): 42 ms
Producer Recovery Time (Node Recovery): 43 ms
Consumer Recovery Time (Node Recovery): 45 ms
===============================================
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 41.13 s -- in com.experiments.KafkaFailoverDockerTest
[INFO]
[INFO] Results:
[INFO]
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  41.681 s
[INFO] Finished at: 2025-04-16T11:01:45+02:00
[INFO] ------------------------------------------------------------------------
